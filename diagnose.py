import torch
import numpy as np
import onnxruntime as ort
from transformers import BertTokenizer, BertForMaskedLM

# ----------------------------
# 1. åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆä¸åŠ ä»»ä½•å¤šä½™å‚æ•°ï¼‰
# ----------------------------
print("ğŸ” æ­£åœ¨åŠ è½½ tokenizer å’Œæ¨¡å‹...")
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
model = BertForMaskedLM.from_pretrained("bert-base-chinese")
model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ----------------------------
# 2. å‡†å¤‡è¾“å…¥ï¼ˆå®é™…é•¿åº¦ï¼Œæ—  paddingï¼‰
# ----------------------------
text = "åŒ—äº¬æ˜¯[MASK]å›½çš„é¦–éƒ½"
inputs = tokenizer(text, return_tensors="pt")
print(f"\nğŸ“ è¾“å…¥æ–‡æœ¬: {text}")
print(f"ğŸ“¥ input_ids shape: {inputs['input_ids'].shape}")
print(f"   input_ids: {inputs['input_ids']}")
print(f"   attention_mask: {inputs['attention_mask']}")

# æ‰¾ [MASK] ä½ç½®
mask_pos = (inputs["input_ids"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()
print(f"ğŸ“ [MASK] ä½ç½®: {mask_pos}")

# ----------------------------
# 3. PyTorch æ¨ç†
# ----------------------------
with torch.no_grad():
    pt_out = model(**inputs)
    pt_logits = pt_out.logits  # [1, seq, vocab]
    pt_probs = torch.softmax(pt_logits[0, mask_pos], dim=-1)
    pt_top5 = torch.topk(pt_probs, 5)

print("\nğŸŸ¢ PyTorch é¢„æµ‹ç»“æœ:")
for i in range(5):
    token_id = pt_top5.indices[i].item()
    prob = pt_top5.values[i].item()
    token = tokenizer.decode([token_id])
    print(f"  {i+1}. '{token}' (id={token_id}, prob={prob:.4f})")

# ----------------------------
# 4. ONNX æ¨ç†
# ----------------------------
try:
    ort_session = ort.InferenceSession("results/onnx/model.onnx")
except Exception as e:
    print(f"âŒ ONNX æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

ort_inputs = {
    "input_ids": inputs["input_ids"].numpy().astype(np.int64),
    "attention_mask": inputs["attention_mask"].numpy().astype(np.int64),
    "token_type_ids": inputs["token_type_ids"].numpy().astype(np.int64),
}

ort_out = ort_session.run(["logits"], ort_inputs)
ort_logits = ort_out[0]  # [1, seq, vocab]
from scipy.special import softmax
ort_probs = softmax(ort_logits[0, mask_pos])
top5_idx = np.argpartition(ort_probs, -5)[-5:]
top5_idx = top5_idx[np.argsort(-ort_probs[top5_idx])]

print("\nğŸ”µ ONNX é¢„æµ‹ç»“æœ:")
for i, idx in enumerate(top5_idx[:5]):
    token = tokenizer.decode([idx])
    print(f"  {i+1}. '{token}' (id={idx}, prob={ort_probs[idx]:.4f})")

# ----------------------------
# 5. æ•°å€¼å¯¹æ¯”
# ----------------------------
pt_np = pt_logits.numpy()
diff = np.abs(pt_np - ort_logits).max()
print(f"\nğŸ“Š æœ€å¤§ logits ç»å¯¹è¯¯å·®: {diff:.6f}")

if diff < 1e-5:
    print("âœ… æ•°å€¼ä¸€è‡´ï¼ONNX å¯¼å‡ºæˆåŠŸ")
else:
    print("âŒ æ•°å€¼ä¸ä¸€è‡´ï¼å¯¼å‡ºæœ‰é—®é¢˜")
    print(f"   PyTorch logits[{mask_pos}, :10] = {pt_np[0, mask_pos, :10]}")
    print(f"   ONNX    logits[{mask_pos}, :10] = {ort_logits[0, mask_pos, :10]}")